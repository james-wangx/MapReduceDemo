# MapReduceDemo

# MapReduce æ¼”ç¤ºæ¡ˆä¾‹

å°šç¡…è°·æœ€å¸…çš„ç”·äººçš„è§†é¢‘ï¼š[https://www.bilibili.com/video/BV1Qp4y1n7EN](https://www.bilibili.com/video/BV1Qp4y1n7EN)

# å…¥é—¨æ¡ˆä¾‹ - è¯é¢‘ç»Ÿè®¡

## ä¸€ã€MapReduce ç¨‹åºç®€å•ç»“æ„
MRï¼ˆå³ MapReduceï¼Œä»¥ä¸‹ç®€ç§° MRï¼‰ç¨‹åºç®€å•å¯ä»¥åˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼š`Mapperï¼ŒReducerï¼ŒDriver`ã€‚ç”¨æˆ·éœ€è¦è‡ªå®šä¹‰ä¸¤ä¸ªç±»ï¼ˆä¸¤ä¸ªæ–‡ä»¶ï¼‰ï¼Œåˆ†åˆ«ç»§æ‰¿ MR API çš„ Mapper å’Œ Reducer ç±»ï¼Œå¹¶ç¼–å†™ main å‡½æ•°å…¥å£å³ Driver æ–‡ä»¶ã€‚å…¶ä¸­ Mapper æ–‡ä»¶å’Œ Reducer æ–‡ä»¶åˆ†åˆ«å¯¹åº” MR ç¨‹åºçš„map å’Œ reduce é˜¶æ®µï¼ŒDriver æ–‡ä»¶ç”¨æ¥é…ç½®ä»»åŠ¡ï¼Œå¦‚æŒ‡å®šè¾“å…¥è¾“å‡ºç±»å‹ï¼Œè¾“å…¥è¾“å‡ºè·¯å¾„ç­‰ã€‚


## äºŒã€WordCountMapper
å…ˆçœ‹ä¸€ä¸‹å®Œæ•´ä»£ç ï¼Œç„¶åå†é€ä¸ªè§£é‡Š
```java
package com.pineapple.mapreduce.wordcount;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

/**
 * Map é˜¶æ®µçš„è¾“å…¥å’Œè¾“å‡ºæ•°æ®éƒ½æ˜¯ KV é”®å€¼å¯¹çš„å½¢å¼ï¼ŒKV çš„ç±»å‹å¯ä»¥è‡ªå®šä¹‰
 * KEYINï¼šmapé˜¶æ®µè¾“å…¥çš„keyçš„ç±»å‹ï¼šLongWritable
 * VALUEINï¼šmapé˜¶æ®µè¾“å…¥valueç±»å‹ï¼šText
 * KEYOUT, mapé˜¶æ®µè¾“å‡ºkeyç±»å‹ï¼šText
 * VALUEOUT, mapé˜¶æ®µè¾“å‡ºvalueç±»å‹ï¼šIntWritable
 */
public class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {

    private final Text outK = new Text();
    private final IntWritable outV = new IntWritable(1);
    private final String regex = ".*?(\\w+-?\\w+).*?";
    private Pattern pattern;
    private Matcher matcher;

    /**
     * é‡å†™ map æ–¹æ³•ï¼Œæ¯ä¸ª KV é”®å€¼å¯¹éƒ½ä¼šè°ƒç”¨ä¸€æ¬¡è¿™ä¸ªæ–¹æ³•
     */
    @Override
    protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, IntWritable>.Context context)
            throws IOException, InterruptedException {
        // value ä¸ºè¿™ä¸€è¡Œçš„æ•°æ®ï¼ŒText ç±»å‹å¯ä»¥è½¬æ¢æˆ Java ä¸­çš„ String ç±»å‹ä»¥è·å–æ›´å¤šæœ‰å…³å­—ç¬¦ä¸²çš„æ“ä½œ
        String line = value.toString();

        // æŒ‰ç…§ç©ºæ ¼åˆ‡å‰²å­—ç¬¦ä¸²
        String[] words = line.split(" ");

        // å¾ªç¯å†™å…¥ç¯å½¢ç¼“å†²åŒº
        for (String word : words) {
            // è¿‡æ»¤æ‰æ ‡ç‚¹ç¬¦å·ï¼ˆä¸åŒ…æ‹¬'-'ï¼‰
            pattern = Pattern.compile(regex);
            matcher = pattern.matcher(word);
            if (matcher.matches()) {
                String group = matcher.group(1);
                // å°è£…outKï¼Œå³å°† String è½¬å°å†™å†è½¬ä¸º Text ç±»å‹
                outK.set(group.toLowerCase());
                // å†™å…¥ï¼Œå‚æ•°ç±»å‹å’Œè¾“å‡ºçš„ KV ç±»å‹ä¸€è‡´
                context.write(outK, outV);
            }
        }
    }
}
```

è‡ªå®šä¹‰ä¸€ä¸ª Java ç±»æ–‡ä»¶ï¼Œåä¸º WordCountMapperã€‚

```java
public class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {

	@Override
	protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, IntWritable>.Context context) {
		......
	}
}
```

ç»§æ‰¿çš„ Mapper æœ‰å››ä¸ªæ³›å‹ï¼Œåˆ†åˆ«å¯¹åº” `map é˜¶æ®µçš„è¾“å…¥ KV å’Œ è¾“å‡º KV`ï¼Œå®ƒä»¬éƒ½æ˜¯ Hadoop è‡ªèº«å°è£…çš„åºåˆ—åŒ–ç±»å‹ï¼Œå’Œ Java çš„æ•°æ®ç±»å‹ç±»ä¼¼ï¼ŒLongWritable ~ Longï¼ŒText ~ Stringï¼ŒIntWritable ~ Integerã€‚

> æ³¨æ„ï¼š
> 	- å› ä¸º Hadoop æœ‰3ä¸ªç‰ˆæœ¬ï¼Œæ‰€ä»¥æœ‰äº›ç±»åæ˜¯é‡å¤çš„ï¼Œå¯¼åŒ…çš„æ—¶å€™å¾ˆå®¹æ˜“å¯¼é”™
> 	Hadoop2.x å’Œ 3.x çš„åŒ…éƒ½æ˜¯ `org.apache.hadoop.mapreduce`ï¼ŒHadoop1.x çš„åŒ…æ˜¯ `org.apache.hadoop.mapred`
> 	- LongWritable, Text, IntWritable ç­‰è¿™äº›æ•°æ®ç±»å‹éƒ½åœ¨ `org.apache.hadoop.io` åŒ…ä¸‹

å…·ä½“çš„ä¸šåŠ¡é€»è¾‘ä»£ç å† map æ–¹æ³•ä¸‹ç¼–å†™ï¼Œkey å’Œ value ä¸ºè¾“å…¥çš„é”®å€¼å¯¹ï¼Œå®ƒä»¬çš„ç±»å‹å’Œ Mapper çš„å‰ä¸¤ä¸ªæ³›å‹æ˜¯ä¸€è‡´çš„ï¼Œcontext å¯ä»¥è§£é‡Šä¸ºä¸Šä¸‹æ–‡å¯¹è±¡ï¼Œä¸»è¦ç”¨æ¥å°†æ•°æ®å†™å…¥ç¯å½¢ç¼“å†²åŒºï¼ˆæ•°æ®å¹¶ä¸æ˜¯ç›´æ¥äº¤ç»™ Reducer å¤„ç†çš„ï¼Œåœ¨è¿™ä¹‹é—´è¿˜æœ‰ä¸ª Shuffle é˜¶æ®µï¼Œç¯å½¢ç¼“å†²åŒºæ˜¯ç”¨æ¥æš‚å­˜æ•°æ®çš„ï¼‰ã€‚

å…³äºè¾“å…¥æ–‡ä»¶ï¼Œæˆ‘é€‰çš„æ˜¯ä¸ªæ¯”è¾ƒå¤æ‚çš„ï¼Œå†…å®¹æ¥è‡ª Hadoop MR å®˜æ–¹æ–‡æ¡£çš„ Overview éƒ¨åˆ†ï¼š[https://hadoop.apache.org/docs/r3.1.3/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html](https://hadoop.apache.org/docs/r3.1.3/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html)
```
Hadoop MapReduce is a software framework for easily writing applications which process vast amounts of data (multi-terabyte data-sets) in-parallel on large clusters (thousands of nodes) of commodity hardware in a reliable, fault-tolerant manner.
A MapReduce job usually splits the input data-set into independent chunks which are processed by the map tasks in a completely parallel manner. The framework sorts the outputs of the maps, which are then input to the reduce tasks. Typically both the input and the output of the job are stored in a file-system. The framework takes care of scheduling tasks, monitoring them and re-executes the failed tasks.
Typically the compute nodes and the storage nodes are the same, that is, the MapReduce framework and the Hadoop Distributed File System (see HDFS Architecture Guide) are running on the same set of nodes. This configuration allows the framework to effectively schedule tasks on the nodes where data is already present, resulting in very high aggregate bandwidth across the cluster.
The MapReduce framework consists of a single master ResourceManager, one worker NodeManager per cluster-node, and MRAppMaster per application (see YARN Architecture Guide).
Minimally, applications specify the input/output locations and supply map and reduce functions via implementations of appropriate interfaces and/or abstract-classes. These, and other job parameters, comprise the job configuration.
The Hadoop job client then submits the job (jar/executable etc.) and configuration to the ResourceManager which then assumes the responsibility of distributing the software/configuration to the workers, scheduling tasks and monitoring them, providing status and diagnostic information to the job-client.
Although the Hadoop framework is implemented in Javaâ„¢, MapReduce applications need not be written in Java.
Hadoop Streaming is a utility which allows users to create and run jobs with any executables (e.g. shell utilities) as the mapper and/or the reducer.
Hadoop Pipes is a SWIG-compatible C++ API to implement MapReduce applications (non JNIâ„¢ based).
```
å¯ä»¥çœ‹åˆ°æ–‡ä»¶ä¸­æœ‰å¾ˆå¤šæ ‡ç‚¹ç¬¦å·ï¼Œå’Œå¸¦ '-' çš„å•è¯ï¼Œç°åœ¨æˆ‘ä»¬çš„ä¸šåŠ¡éœ€æ±‚æ˜¯è¿‡æ»¤æ‰æ ‡ç‚¹ç¬¦å·ï¼Œä¿ç•™å¸¦ '-' çš„å•è¯ï¼Œæœ€ç»ˆç»Ÿè®¡è¯é¢‘ã€‚

æˆ‘ä»¬é‡ç‚¹æ“ä½œçš„æ˜¯ value å¯¹è±¡ï¼Œå®ƒä»£è¡¨çš„æ˜¯æ–‡ä»¶ä¸­çš„ä¸€è¡Œæ•°æ®ã€‚ç”±äº Hadoop çš„ Text ç±»å‹å¯¹å­—ç¬¦ä¸²çš„æ“ä½œæ¯”è¾ƒå°‘ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†å…¶è½¬ä¸º String ç±»å‹è¿›è¡Œæ“ä½œã€‚éšåæŒ‰ç…§ç©ºæ ¼å°†è¿™ä¸€è¡Œåˆ‡å‰²æˆä¸€ä¸ªä¸ªå•è¯ã€‚
```java
// value ä¸ºè¿™ä¸€è¡Œçš„æ•°æ®ï¼ŒText ç±»å‹å¯ä»¥è½¬æ¢æˆ Java ä¸­çš„ String ç±»å‹ä»¥è·å–æ›´å¤šæœ‰å…³å­—ç¬¦ä¸²çš„æ“ä½œ
String line = value.toString();
// æŒ‰ç…§ç©ºæ ¼åˆ‡å‰²å­—ç¬¦ä¸²
String[] words = line.split(" ");
```
ç”¨ for each è¯­å¥éå†è¿™ä¸€è¡Œçš„æ¯ä¸ªå•è¯ï¼Œç”¨æ­£åˆ™è¡¨è¾¾å¼è¿‡æ»¤åå†è½¬ä¸ºå°å†™ã€‚æœ€åå†™å…¥ç¯å½¢ç¼“å†²åŒºã€‚
```java
// å¾ªç¯å†™å…¥ç¯å½¢ç¼“å†²åŒº
for (String word : words) {
    // è¿‡æ»¤æ‰æ ‡ç‚¹ç¬¦å·ï¼ˆä¸åŒ…æ‹¬'-'ï¼‰
    pattern = Pattern.compile(regex);
    matcher = pattern.matcher(word);
    if (matcher.matches()) {
        String group = matcher.group(1);
        // å°è£…outKï¼Œå³å°† String è½¬å°å†™å†è½¬ä¸º Text ç±»å‹
        outK.set(group.toLowerCase());
        // å†™å…¥ï¼Œå‚æ•°ç±»å‹å’Œè¾“å‡ºçš„ KV ç±»å‹ä¸€è‡´
        context.write(outK, outV);
    }
}
```
> æ³¨æ„ï¼š
> - æœ€ç»ˆ context å†™å…¥çš„æ•°æ®ç±»å‹å¿…é¡»å’Œ Mapper çš„åä¸¤ä¸ªæ³›å‹ä¸€è‡´
> - map æ–¹æ³•æ¯è¯»å–ä¸€è¡Œå°±ä¼šè°ƒç”¨ä¸€æ¬¡ï¼Œä¸ºäº†é¿å…å†…å­˜å¼€é”€ï¼Œå¯ä»¥å°† Textï¼ŒIntWritable ç­‰ä¸€äº›ç±»å‹çš„å£°æ˜æ”¾åœ¨ map æ–¹æ³•å¤–


## ä¸‰ã€WordCountReducer
å…ˆçœ‹ä¸€ä¸‹å®Œæ•´ä»£ç ï¼Œå†é€ä¸ªè§£é‡Š
```java
package com.pineapple.mapreduce.wordcount;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;

/**
 * Reduce é˜¶æ®µçš„è¾“å…¥æ•°æ®ç±»å‹è¦å’Œ Map é˜¶æ®µçš„è¾“å‡ºæ•°æ®ç±»å‹å¯¹åº”
 * KEYIN, reduceé˜¶æ®µè¾“å…¥çš„keyçš„ç±»å‹ï¼šText
 * VALUEIN, reduceé˜¶æ®µè¾“å…¥valueç±»å‹ï¼šIntWritable
 * KEYOUT, reduceé˜¶æ®µè¾“å‡ºkeyç±»å‹ï¼šText
 * VALUEOUT, reduceé˜¶æ®µè¾“å‡ºvalueç±»å‹ï¼šIntWritable
 */
public class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {

    private final IntWritable outV = new IntWritable();

    /**
     * æ¯ä¸€ä¸ª Key éƒ½ä¼šè°ƒç”¨æ­¤æ–¹æ³•
     */
    @Override
    protected void reduce(Text key, Iterable<IntWritable> values, Reducer<Text, IntWritable, Text, IntWritable>.Context
            context) throws IOException, InterruptedException {

        int sum = 0;

        // å€¼ values æ˜¯ä¸€ä¸ªè¿­ä»£å™¨ï¼Œå¯ä»¥ä½¿ç”¨ for each å¾ªç¯å–å‡ºæ¯ä¸€é¡¹
        for (IntWritable value : values)
            sum += value.get();

        outV.set(sum);

        // å†™å…¥æ–‡ä»¶
        context.write(key, outV);
    }
}
```
è‡ªå®šä¹‰ä¸€ä¸ª Java ç±»æ–‡ä»¶ï¼Œç±»åä¸º WordCountReducerã€‚
```java
public class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
	
	@Override
    protected void reduce(Text key, Iterable<IntWritable> values, Reducer<Text, IntWritable, Text, IntWritable>.Context
            context) {
		......
	}
}
```
Reduer çš„å‰ä¸¤ä¸ªæ³›å‹ä¸ºä¹‹å‰ Mapper è¾“å‡ºçš„ä¸¤ä¸ªç±»å‹ï¼Œåä¸¤ä¸ªç±»å‹ä¸ºæœ€ç»ˆå†™å…¥ç»“æœæ–‡ä»¶çš„ç±»å‹ã€‚
è¿™é‡Œçš„æ•°æ®åœ¨ç»è¿‡ Shuffle é˜¶æ®µåï¼Œå·²ç»ç”±<key, 1>çš„å½¢å¼å˜æˆäº†<key, <1, 1, 1>>çš„å½¢å¼ï¼Œæ‰€ä»¥ values æ˜¯ä¸€ä¸ª`å¯è¿­ä»£å¯¹è±¡`ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ for each å¾ªç¯å°†æ¯ä¸ª1åŠ èµ·æ¥å¾—åˆ°æœ€ç»ˆçš„å•è¯é¢‘ç‡ã€‚
```java
int sum = 0;

// å€¼ values æ˜¯ä¸€ä¸ªè¿­ä»£å™¨ï¼Œå¯ä»¥ä½¿ç”¨ for each å¾ªç¯å–å‡ºæ¯ä¸€é¡¹
for (IntWritable value : values)
    sum += value.get();

outV.set(sum);

// å†™å…¥æ–‡ä»¶
context.write(key, outV);
```
> æ³¨æ„ï¼š
> - æ¯ä¸€ä¸ªé”®ä¸åŒçš„é”®å€¼å¯¹éƒ½å¯¹è°ƒç”¨ä¸€æ¬¡ reduce æ–¹æ³•ï¼Œä¸ºäº†é¿å…å†…å­˜å¼€é”€ï¼Œå¯ä»¥å°†æ•°æ®ç±»å‹çš„å®šä¹‰æ”¾åœ¨ reduce æ–¹æ³•å¤–ã€‚
> - context å†™å…¥çš„æ•°æ®ç±»å‹å’Œ Reducer çš„åä¸¤ä¸ªæ³›å‹çš„ç±»å‹ä¸€è‡´ã€‚


## å››ã€WordCountDriver
å®Œæ•´ä»£ç 
```java
package com.pineapple.mapreduce.wordcount;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

public class WordCountDriver {

    public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {

        // åˆ›å»ºä¸€ä¸ªæ–°çš„ job
        // å¦‚æœè¦ new çš„ Configuration() ä¸ºç©ºï¼Œåˆ™å¯ä»¥ç›´æ¥è°ƒç”¨ getInstance()
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf);
        job.setJarByClass(WordCountDriver.class);

        // æŒ‡å®šä¸€ä¸‹ job å
        job.setJobName("WordCount");

        // å…³è” mapper å’Œ reducer
        job.setMapperClass(WordCountMapper.class);
        job.setReducerClass(WordCountReducer.class);

        // è®¾ç½® map è¾“å‡ºçš„ kv ç±»å‹
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(IntWritable.class);

        // è®¾ç½®æœ€ç»ˆè¾“å‡ºçš„ kv ç±»å‹
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        // åˆ¤æ–­è¾“å‡ºè·¯å¾„æ˜¯å¦å­˜åœ¨ï¼Œå­˜åœ¨åˆ™åˆ é™¤
        FileSystem fileSystem = FileSystem.get(conf);
        Path outputPath = new Path("output/WordCount");
        if (fileSystem.exists(outputPath))
            fileSystem.delete(outputPath, true);

        // è®¾ç½®è¾“å…¥è·¯å¾„å’Œè¾“å‡ºè·¯å¾„
        FileInputFormat.setInputPaths(job, new Path("input/WordCount"));
        FileOutputFormat.setOutputPath(job, outputPath);

        // æäº¤job
        boolean result = job.waitForCompletion(true);
        System.exit(result ? 0 : 1);
    }
}
```
Driver çš„ä»£ç éƒ½æ˜¯ä¸€äº›å›ºå®šæ ¼å¼ï¼Œè¦è®²çš„åŸºæœ¬éƒ½åœ¨æ³¨é‡Šé‡Œäº†ã€‚Driver ä¸»è¦è¿˜æ˜¯èµ·åˆ°äº†ä¸€ä¸ªé…ç½®çš„ä½œç”¨ï¼Œè‡ªå®šä¹‰çš„ Mapper å’Œ Reducer ï¼Œè¾“å…¥è¾“å‡ºæ•°æ®ç±»å‹ï¼Œè¾“å…¥è¾“å‡ºè·¯å¾„ç­‰æ˜¯è¦æŒ‡å®šä¸€ä¸‹çš„ï¼Œä¸ç„¶ MR æ‰¾ä¸åˆ°å®ƒä»¬ã€‚å°†å®ƒä»¬çš„ `class` ä¼ å…¥ï¼Œåœ¨è¿è¡Œ MR ç¨‹åºæ—¶ï¼ŒJava ä¼šé€šè¿‡`åå°„çš„æœºåˆ¶`è°ƒç”¨æˆ‘ä»¬å†™çš„ä»£ç ã€‚


## äº”ã€è·‘ä¸€ä¸‹
ç›´æ¥è¿è¡Œ Driver åä¼šåœ¨è¾“å‡ºè·¯å¾„äº§å‡ºç»“æœæ–‡ä»¶ï¼Œå†…å®¹å¤§æ¦‚å¦‚ä¸‹ï¼š
```
abstract-classes	1
across	1
aggregate	1
allows	2
already	1
although	1
amounts	1
and	14
......
```
å¦‚æœæ˜¯è¦æäº¤çš„é›†ç¾¤ä¸Šè¿è¡Œï¼Œå¯ä»¥åˆ©ç”¨ä¸€ä¸‹å‚æ•° `args` ï¼Œé€šè¿‡æŒ‡å®šå‚æ•°è®¾ç½®æ–‡ä»¶çš„è¾“å…¥è¾“å‡ºè·¯å¾„ï¼š
```java
// åˆ¤æ–­è¾“å‡ºè·¯å¾„æ˜¯å¦å­˜åœ¨ï¼Œå­˜åœ¨åˆ™åˆ é™¤
Path inputPath = new Path(args[0]);
Path outputPath = new Path(args[1]);
FileSystem fileSystem = FileSystem.get(conf);
if (fileSystem.exists(outputPath))
    fileSystem.delete(outputPath, true);

// è®¾ç½®è¾“å…¥è·¯å¾„å’Œè¾“å‡ºè·¯å¾„
FileInputFormat.setInputPaths(job, inputPath);
FileOutputFormat.setOutputPath(job, outputPath);
```

æ‰“åŒ…åˆ°é›†ç¾¤ä¸Šè¿è¡Œï¼Œç»“æœå¦‚ä¸‹ï¼š
```bash
hadoop jar MapReduceDemo-1.0-SNAPSHOT.jar com.pineapple.mapreduce.wordcount.HdfsWordCountDriver hdfs://hadoop102:8020/input/WordCount hdfs://hadoop102:8020/output/WordCount
```
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/b5ceff9ca32c4d7aad0fa1ba5031b798.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcGluZWFwcGxlX3B5,size_20,color_FFFFFF,t_70,g_se,x_16)

-----

Githubä»“åº“åœ°å€ï¼š[https://github.com/pineapple-cpp/MapReduceDemo](https://github.com/pineapple-cpp/MapReduceDemo)

å–œæ¬¢æˆ‘çš„æ–‡ç« çš„è¯ï¼Œæ¬¢è¿`å…³æ³¨`ğŸ‘‡`ç‚¹èµ`ğŸ‘‡`è¯„è®º`ğŸ‘‡`æ”¶è—`ğŸ‘‡	è°¢è°¢æ”¯æŒï¼ï¼ï¼